# PDF RAG Assistant (Ingestion Foundation)
A full-stack application that allows authenticated users to upload PDF documents and query their content using a Retrieval-Augmented Generation (RAG) pipeline.

This project focuses on clean system design, asynchronous processing, and secure integration between frontend, backend, and AI workflows.

The document ingestion pipeline is fully implemented and operational; query and UI layers are under active development.

## ğŸš€ Key Features
- User authentication via Clerk
- Secure document upload
- Asynchronous ingestion using a queue + background worker with batch-level logging and retry-safe vector indexing
- Multi-format document support (PDF, HTML, DOCX, TXT)
- Robust document cleaning & normalization
- Exact and near-duplicate content detection
- Idempotent ingestion (duplicate uploads are automatically skipped)
- Deterministic chunking
- Vector embeddings generation
- Vector storage in Pinecone
- Modular, production-grade architecture

âš ï¸ This project currently focuses on document ingestion only.
Querying and LLM-based responses are planned but not yet implemented.

## Ingestion Pipeline
The ingestion pipeline explicitly implements the following stages:
```text
Raw Document
 â†’ Text Extraction (format-aware)
 â†’ Boilerplate Removal
 â†’ Text Normalization
 â†’ Exact & Near-Deduplication
 â†’ Chunking (deterministic)
 â†’ Metadata Enrichment
 â†’ Quality Validation (guardrails + skip semantics)
 â†’ Embeddings (LangChain-managed batching)
 â†’ Batched Vector Indexing (retry-safe, observable)
```
This ensures high-quality, deduplicated, and retrieval-ready data while preventing runaway cost.

## ğŸ›¡ Ingestion Guardrails & Cost Control
- The ingestion system enforces explicit safeguards to ensure predictable behavior:
- Hard chunk limits to prevent runaway documents
- Low-information chunk filtering
- Skip semantics for invalid documents (non-retryable)
- Idempotent ingestion using content fingerprints (Redis-backed)
- Batched Pinecone indexing with isolated retries
- Namespace isolation to prevent cross-user data leakage
Invalid documents are intentionally skipped, not failed.

## ğŸ— Architecture Overview
```text
Next.js (Frontend UI)
        â†“
Express API (Auth, Upload, Queue)
        â†“
Valkey / Redis (persistent)
  â”œâ”€ BullMQ (job queue)
  â”œâ”€ Fingerprint store (idempotent ingestion)
  â””â”€ Shared Redis client (single connection)
        â†“
Background Worker
        â”œâ”€ Extraction (PDF / HTML / DOCX / TXT)
        â”œâ”€ Cleaning & Normalization
        â”œâ”€ Deduplication
        â”œâ”€ Chunking
        â”œâ”€ Validation
        â”œâ”€ Embeddings
        â””â”€ Pinecone Indexing
```
- Authentication is enforced at the API boundary
- All heavy processing is offloaded to background workers
- Workers are concurrency-limited and retry-safe

## Tech Stack
### Frontend
- Next.js
- TypeScript
- Clerk Authentication

### Backend
- Node.js
- Express
- Multer
- BullMQ (queue + worker)
- Valkey / Redis

### AI / Vector Infrastructure
- LangChain (modular usage)
- Hugging Face Inference embeddings
- Pinecone (vector database)

### Infrastructure
- Docker
- Docker Compose

## ğŸ“ Project Structure
```bash
pdf_rag_assistant
â”œâ”€â”€ server
â”‚   â”œâ”€â”€ index.js                 # API server
â”‚   â”œâ”€â”€ worker.js                # Background worker
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ extract/             # PDF, HTML, DOCX, TXT extractors
â”‚   â”‚   â”œâ”€â”€ clean/               # Boilerplate + normalization
â”‚   â”‚   â”œâ”€â”€ dedupe/              # Exact & near-duplicate detection
â”‚   â”‚   â”œâ”€â”€ chunk/               # Deterministic chunking
â”‚   â”‚   â”œâ”€â”€ enrich/              # Metadata attachment
â”‚   â”‚   â”œâ”€â”€ validate/            # Quality checks & guardrails
â”‚   â”‚   â”œâ”€â”€ embed/               # Embeddings
â”‚   â”‚   â”œâ”€â”€ index/               # Pinecone indexing
â”‚   â”‚   â””â”€â”€ pipeline.js          # Canonical ingestion pipeline
â”‚   â”œâ”€â”€ utils/                   # Shared utilities
â”‚   â”‚   â”œâ”€â”€ batch.util.js        # Network-bound batching
â”‚   â”‚   â”œâ”€â”€ retry.util.js        # Isolated retry logic
â”‚   â”‚   â”œâ”€â”€ fingerprint.util.js  # Content fingerprinting
â”‚   â”‚   â””â”€â”€ fingerprint.store.js # Redis-backed idempotency
â”‚   â””â”€â”€ config/
â”œâ”€â”€ client/web                   # Next.js app
â”œâ”€â”€ scripts
â”œâ”€â”€ docker-compose.yml

```

## Local Setup
### Prerequisites
- Node.js (v18+)
- Docker

### Environment Variables
Create environment files using the provided .env.example templates in the respective client and server directories.
A single Redis connection is shared by BullMQ and all ingestion utilities (batch retries, fingerprint storage, idempotency).

## Redis / Valkey Persistence
Redis / Valkey is used not only for job queueing (BullMQ) but also for
persistent ingestion state such as document fingerprints.

To ensure idempotent ingestion across restarts, Redis persistence **must be enabled**. The system relies on Redis retaining keys between container or process restarts.

Persistence is typically enabled via:
- RDB snapshots (recommended)
- or AOF (append-only file)

When running via Docker, a volume must be mounted to persist data.

## Running the Application
From the project root, run:
```bash
npm install
npm run dev
```

This will:
- Start required Docker services
- Start the backend API
- Start the background worker
- Start the frontend application

## ğŸ”„ Background Worker Design
- Workers consume jobs from the file-upload-queue
- Concurrency is intentionally limited
- Files are read using absolute, normalized paths
- PDF extraction uses buffer-based loading for cross-platform safety
- Batch-level retries occur only where failures happen
- Invalid documents are skipped, not retried

## ğŸ“Œ Current Project Status
| Feature                       | Status             |
| ----------------------------- | -------------------|
| Authentication & uploads      | âœ…                 |
| Asynchronous ingestion        | âœ…                 |
| Multi-format extraction       | âœ…                 |
| Cleaning & deduplication      | âœ…                 |
| Embeddings & Pinecone storage | âœ…                 |
| User prompt handling          | âŒ Not implemented |
| RAG querying                  | âŒ Not implemented |
| LLM responses                 | âŒ Not implemented |


## ğŸ§­ Design Philosophy
- Clear separation of concerns (API, worker, pipeline)
- Deterministic and retry-safe ingestion
- Infrastructure decoupled from business logic
- Idempotency relies on persistent state; infrastructure durability is treated as a correctness requirement, not an optimization
- Prefer no ingestion over incorrect ingestion
- Built as a scalable foundation for future RAG capabilities
- A single shared Redis client is used to enforce consistency and prevent connection sprawl

## Batching & Retry Strategy
- Batching is applied only at network boundaries
- Embeddings are batched internally by LangChain
- Pinecone indexing is explicitly batched at the application level
- Each batch is logged with start / success / failure states
- Retries occur only at the failing batch, not the entire document
- Failures propagate cleanly for queue-level retries

## ğŸ”® Planned Enhancements
- User query & prompt processing
- Retrieval pipeline
- LLM-based answer generation
- Citation & source attribution
- Evaluation & feedback loop

## ğŸ“ Summary
This project provides a production-ready ingestion foundation for a RAG system.

The pipeline is idempotent, cost-aware, retry-safe, and designed to be safely run multiple times without duplicating data or incurring unbounded embedding costs.