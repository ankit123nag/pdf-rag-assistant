# PDF RAG Assistant (Ingestion Foundation)
A full-stack application that allows authenticated users to upload PDF documents and query their content using a Retrieval-Augmented Generation (RAG) pipeline.

This project focuses on clean system design, asynchronous processing, and secure integration between frontend, backend, and AI workflows.

The document ingestion pipeline is fully implemented and operational; query and UI layers are under active development.

## ğŸš€ Key Features
- User authentication via Clerk
- Secure document upload
- Asynchronous ingestion using a queue + background worker
- Multi-format document support (PDF, HTML, DOCX, TXT)
- Robust document cleaning & normalization
- Exact and near-duplicate content detection
- Deterministic chunking
- Vector embeddings generation
- Vector storage in Pinecone
- Modular, production-grade architecture

âš ï¸ This project currently focuses on document ingestion only.
Querying and LLM-based responses are planned but not yet implemented.

## Ingestion Pipeline
The ingestion pipeline explicitly implements the following stages:
```text
Raw Document
 â†’ Text Extraction (format-aware)
 â†’ Boilerplate Removal
 â†’ Text Normalization
 â†’ Exact & Near-Deduplication
 â†’ Chunking
 â†’ Metadata Enrichment
 â†’ Quality Validation
 â†’ Embeddings
 â†’ Vector Indexing
```
This ensures high-quality, deduplicated, and retrieval-ready data.

## ğŸ— Architecture Overview
```text
Next.js (Frontend UI)
        â†“
Express API (Auth, Upload, Queue)
        â†“
Valkey / Redis Queue
        â†“
Background Worker
        â”œâ”€ Extraction (PDF / HTML / DOCX / TXT)
        â”œâ”€ Cleaning & Normalization
        â”œâ”€ Deduplication
        â”œâ”€ Chunking
        â”œâ”€ Embeddings
        â””â”€ Pinecone Indexing
```
- Authentication is enforced at the API boundary
- All heavy processing is offloaded to background workers
- Workers are retry-safe and fail fast on invalid data

## Tech Stack
### Frontend
- Next.js
- TypeScript
- Clerk Authentication

### Backend
- Node.js
- Express
- Multer
- BullMQ (queue + worker)
- Valkey / Redis

### AI / Vector Infrastructure
- LangChain (modular usage)
- Hugging Face Inference embeddings
- Pinecone (vector database)

### Infrastructure
- Docker
- Docker Compose

## ğŸ“ Project Structure
```bash
pdf_rag_assistant
â”œâ”€â”€ server
â”‚   â”œâ”€â”€ index.js                # API server
â”‚   â”œâ”€â”€ worker.js               # Background worker
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ extract/            # PDF, HTML, DOCX, TXT extractors
â”‚   â”‚   â”œâ”€â”€ clean/              # Boilerplate + normalization
â”‚   â”‚   â”œâ”€â”€ dedupe/             # Exact & near-duplicate detection
â”‚   â”‚   â”œâ”€â”€ chunk/              # Deterministic chunking
â”‚   â”‚   â”œâ”€â”€ enrich/             # Metadata attachment
â”‚   â”‚   â”œâ”€â”€ validate/           # Quality checks
â”‚   â”‚   â”œâ”€â”€ embed/              # Embeddings
â”‚   â”‚   â”œâ”€â”€ index/              # Pinecone indexing
â”‚   â”‚   â””â”€â”€ pipeline.js         # Canonical ingestion pipeline
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ detectDocumentType.js
â”‚   â””â”€â”€ config/
â”œâ”€â”€ client/web                  # Next.js app
â”œâ”€â”€ scripts
â”œâ”€â”€ docker-compose.yml

```

## Local Setup
### Prerequisites
- Node.js (v18+)
- Docker

### Environment Variables
Create environment files using the provided .env.example templates in the respective client and server directories.

## Running the Application
From the project root, run:
```bash
npm install
npm run dev
```

This will:
- Start required Docker services
- Start the backend API
- Start the background worker
- Start the frontend application

## ğŸ”„ Background Worker Design
- Workers consume jobs from the file-upload-queue
- Concurrency is managed by BullMQ
- Files are read using absolute, normalized paths
- PDF extraction uses buffer-based loading for cross-platform safety
- Failures propagate correctly for retries

## ğŸ“Œ Current Project Status
| Feature                       | Status             |
| ----------------------------- | -------------------|
| Authentication & uploads      | âœ…                 |
| Asynchronous ingestion        | âœ…                 |
| Multi-format extraction       | âœ…                 |
| Cleaning & deduplication      | âœ…                 |
| Embeddings & Pinecone storage | âœ…                 |
| User prompt handling          | âŒ Not implemented |
| RAG querying                  | âŒ Not implemented |
| LLM responses                 | âŒ Not implemented |


## ğŸ§­ Design Philosophy
- Clear separation of concerns (API, worker, pipeline)
- Deterministic and retry-safe ingestion
- Infrastructure decoupled from business logic
- Built as a scalable foundation for future RAG capabilities

## ğŸ”® Planned Enhancements
- User query & prompt processing
- Retrieval pipeline
- LLM-based answer generation
- Citation & source attribution
- Evaluation & feedback loop

## ğŸ“ Summary
This project serves as a robust foundation for a RAG system, focusing first on ingestion quality and system correctness before adding query and generation layers.